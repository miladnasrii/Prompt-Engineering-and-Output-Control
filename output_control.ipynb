{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "Dg8QyEPdFoGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Summarization Example**"
      ],
      "metadata": {
        "id": "0fyxp8uIFhPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "G9yIpejQKGqZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "78xqTgobIgok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8CmFOVRJTUC"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Explainable Artificial Intelligence (XAI) is a set of processes and\n",
        "methods that allows human users to comprehend and trust the results and\n",
        "output created by machine learning algorithms.\"\"\"\n",
        "\n",
        "summary = summarizer(text, max_length=30, min_length=10, do_sample=False)\n",
        "\n",
        "print(summary[0]['summary_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Creative Story Generation Example**"
      ],
      "metadata": {
        "id": "VnhNIL35GJI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers"
      ],
      "metadata": {
        "id": "pSwNRHhd3Oab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "metadata": {
        "id": "YgeLmjxAJTDn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "7xof93eYJRVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_story(prompt_text, max_length=100, temperature=0.7):\n",
        "    inputs = tokenizer.encode(prompt_text, return_tensors=\"pt\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "story_template = \"\"\"Create a short story based on the following prompt:\n",
        "Prompt: \"{prompt}\"\n",
        "Story:\"\"\"\n",
        "\n",
        "def create_story(prompt_input):\n",
        "    prompt_text = story_template.format(prompt=prompt_input)\n",
        "    return generate_story(prompt_text)\n",
        "\n",
        "prompt_input = \"The person who invites Geoffrey Hinton to give the Nobel Prize in the TV show.\"\n",
        "\n",
        "story = create_story(prompt_input)\n",
        "print(story)"
      ],
      "metadata": {
        "id": "VHTE68gtGHsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question Answering Example**"
      ],
      "metadata": {
        "id": "a-5sthPOGMiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForQuestionAnswering, BertTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "3yCWL7DqL4w7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "i8vdDVIDL7Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(context, question):\n",
        "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    answer_start = torch.argmax(outputs.start_logits)\n",
        "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "    return answer\n",
        "\n",
        "qa_template = \"\"\"You are an expert assistant. Answer the following question based on the provided context:\n",
        "Context: \"{context}\"\n",
        "Question: \"{question}\"\n",
        "Answer:\"\"\"\n",
        "\n",
        "def generate_answer(context, question):\n",
        "    return answer_question(context, question)\n",
        "\n",
        "context = \"Explainable Artificial Intelligence (XAI) is a set of processes and methods that allows human users to comprehend and trust the results and output created by machine learning algorithms.\"\n",
        "question = \"What is Explainable Artificial Intelligence?\"\n",
        "\n",
        "answer = generate_answer(context, question)\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "xb83aClSGPx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Code Generation Example**"
      ],
      "metadata": {
        "id": "PLuLtHNxGTPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "bcVameA-Mdng"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "-lihfY6dMgTD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_code(task_description, max_length=180, temperature=0.6):\n",
        "    prompt_text = f\"Task: {task_description}\\nPython Code:\"\n",
        "\n",
        "    inputs = tokenizer.encode(prompt_text, return_tensors=\"pt\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    code_start = generated_code.find(\"Python Code:\") + len(\"Python Code:\")\n",
        "    return generated_code[code_start:].strip()\n",
        "\n",
        "task_description = \"Define a function in Python that takes a number as input and prints its square.\"\n",
        "\n",
        "generated_code = generate_code(task_description)\n",
        "print(generated_code)"
      ],
      "metadata": {
        "id": "HBfQPov3GTzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Formal Letter Writing Example**"
      ],
      "metadata": {
        "id": "Ev9tHKnAGXIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "Ai9l82BFNGGv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "4lGbqKrhNGnu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_letter(sender_name, recipient_name, reason, tone, max_length=100, temperature=0.8):\n",
        "    prompt_text = f\"\"\"Write a formal letter for the following scenario:\n",
        "Sender Name: \"{sender_name}\"\n",
        "Recipient Name: \"{recipient_name}\"\n",
        "Reason for Writing: \"{reason}\"\n",
        "Tone: \"{tone}\"\n",
        "Letter:\"\"\"\n",
        "\n",
        "    inputs = tokenizer.encode(prompt_text, return_tensors=\"pt\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_letter = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    letter_start = generated_letter.find(\"Letter:\") + len(\"Letter:\")\n",
        "    return generated_letter[letter_start:].strip()\n",
        "\n",
        "sender_name = \"Milad Nasri\"\n",
        "recipient_name = \"Dr. Moradi\"\n",
        "reason = \"Meeting at the RMIT university\"\n",
        "tone = \"Academic\"\n",
        "\n",
        "generated_letter = generate_letter(sender_name, recipient_name, reason, tone)\n",
        "print(generated_letter)"
      ],
      "metadata": {
        "id": "R3n_qVlZGXfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Paraphrasing Example**"
      ],
      "metadata": {
        "id": "q6qPFkrwGe_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "WyhSZmqXOURH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "j1RExYhuOWCf"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def paraphrase_sentence(sentence, max_length=70, temperature=0.6):\n",
        "    prompt_text = f\"\"\"Paraphrase the following sentence to make it clearer and more concise:\n",
        "Sentence: \"{sentence}\"\n",
        "Paraphrased Sentence:\"\"\"\n",
        "\n",
        "    inputs = tokenizer.encode(prompt_text, return_tensors=\"pt\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_paraphrase = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    paraphrase_start = generated_paraphrase.find(\"Paraphrased Sentence:\") + len(\"Paraphrased Sentence:\")\n",
        "    return generated_paraphrase[paraphrase_start:].strip()\n",
        "\n",
        "sentence = \"Explainable Artificial Intelligence (XAI) is a set of processes and methods that allows human users to comprehend and trust the results and output created by machine learning algorithms.\"\n",
        "\n",
        "paraphrased_sentence = paraphrase_sentence(sentence)\n",
        "print(paraphrased_sentence)"
      ],
      "metadata": {
        "id": "neR-XtACGfl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kvCg_pjtOXYw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}